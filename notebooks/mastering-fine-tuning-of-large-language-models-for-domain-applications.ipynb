{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ Interactive Article Notebook\n\n**Title:** Mastering Fine-Tuning of Large Language Models for Domain Applications\n\n**Description:** Unlock the potential of large language models with Hugging Face Transformers. Learn step-by-step fine-tuning for domain-specific applications, from data prep to deployment.\n\n**ðŸ“– Read the full article:** [Mastering Fine-Tuning of Large Language Models for Domain Applications](https://blog.thegenairevolution.com/article/mastering-fine-tuning-of-large-language-models-for-domain-applications)\n\n---\n\n*This interactive notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So here's the thing - building a chatbot that actually remembers what you talked about five minutes ago isn't as complicated as you might think. I recently dove into this exact problem using Hugging Face Transformers and LangChain, and honestly, the results were better than I expected. Let me walk you through how to build one yourself.\n\n## Installation\nFirst things first, you'll need to get the libraries installed. Nothing fancy here - just run this command and you're good to go:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary libraries for building a memory-aware chatbot\n!pip install transformers langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Setup\nNow let's get our environment ready. We're going to use GPT-2 for this tutorial - it's lightweight enough to run on most machines but still powerful enough to have decent conversations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Define the model name for the chatbot\nmodel_name = \"gpt2\"\n\n# Load the pre-trained model for causal language modeling\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Load the tokenizer associated with the pre-trained model\ntokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step-by-Step Build\n### Data Handling\nHere's where things get interesting. To give our chatbot memory, we need somewhere to store the conversation history. I'm keeping it simple with a basic list - nothing fancy, but it works surprisingly well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize an empty list to store conversation history\nconversation_history = []\n\ndef add_to_history(user_input, bot_response):\n    # Append the user input and bot response to the conversation history\n    conversation_history.append({\"user\": user_input, \"bot\": bot_response})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Actually, wait - let me clarify something important here. This approach stores everything in memory during runtime, which means when you restart your script, the conversation history disappears. For a production system, you'd want to persist this to a database or file. But for learning purposes, this works perfectly.\n\n### Model Integration\nThis is where the magic happens. We're going to make the model aware of previous conversations by feeding it the history along with the current input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(user_input):\n    # Tokenize the input and conversation history\n    input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n    history_ids = tokenizer.encode(\" \".join([entry[\"user\"] + entry[\"bot\"] for entry in conversation_history]), return_tensors='pt')\n\n    # Concatenate history and input for context-aware generation\n    input_ids = torch.cat((history_ids, input_ids), dim=-1)\n\n    # Generate a response using the model\n    output = model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n    response = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n\n    # Add the interaction to the history\n    add_to_history(user_input, response)\n\n    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full End-to-End Application\nLet's put it all together. Here's the complete script that you can run right away:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full script for a memory-aware chatbot\nimport torch\n\ndef chat():\n    print(\"Start chatting with the bot (type 'exit' to stop)!\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            break\n        response = generate_response(user_input)\n        print(f\"Bot: {response}\")\n\n# Start the chat\nchat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing & Validation\nWhen you run this, you'll get a simple command-line interface where you can chat with your bot. Try something like this:\n\nStart with \"Hello, who are you?\" and the bot might respond with something like \"I am a chatbot created to assist you.\"\n\nThen ask \"What can you do?\" - and here's the cool part - the bot will remember the context of your previous question and give you a more relevant answer like \"I can chat with you and remember our past conversations.\"\n\nThe more you chat, the more context it builds up. But here's something I learned the hard way: be careful with really long conversations. The model has a token limit, and once you hit it, things can get weird. You might need to implement a sliding window approach where you only keep the last N exchanges in memory.\n\n## Conclusion\nBuilding this memory-aware chatbot taught me a lot about how context shapes conversations in AI. The basic version we built here is surprisingly effective for simple use cases, but there's so much room for improvement.\n\nIf you want to take this further, consider adding more sophisticated memory management - maybe store important facts separately from casual conversation. You could also deploy this on a cloud platform for real-world use. And honestly, once you start playing with different models (try GPT-3 or Claude if you have access), the quality jump is remarkable.\n\nThe biggest challenge I found wasn't the technical implementation - it was managing the conversation flow in a way that felt natural. But that's a problem for another tutorial."
      ]
    }
  ],
  "metadata": {
    "title": "Interactive Article Tutorial: Mastering Fine-Tuning of Large Language Models for Domain Applications",
    "description": "Unlock the potential of large language models with Hugging Face Transformers. Learn step-by-step fine-tuning for domain-specific applications, from data prep to deployment.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}